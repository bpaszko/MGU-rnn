{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import torch.utils.data as data\n",
    "import os\n",
    "import pretty_midi\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import deque, Counter\n",
    "from typing import Dict, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = '/home/bartek/Datasets/maestro/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def midi_path_to_pianoroll(path: str, fs: int=5) -> np.ndarray:\n",
    "    pmid = pretty_midi.PrettyMIDI(path)\n",
    "    piano = pmid.instruments[0]\n",
    "    pianoroll = piano.get_piano_roll(fs=fs)\n",
    "    return pianoroll\n",
    "    \n",
    "def pianoroll_to_time_dict(pianoroll: np.ndarray) -> Dict[int, str]:\n",
    "    times = np.unique(pianoroll.nonzero()[1])  # czasy gdzie występuje przynajmniej jedna nuta \n",
    "    index = pianoroll.nonzero()  # indeksy wszystkich nut\n",
    "    dict_keys_time = {}\n",
    "\n",
    "    for time in times:\n",
    "        index_where = (index[1] == time).nonzero()  # pozycje nut, które występują w danym czasie, w indeksie\n",
    "        notes = index[0][index_where]  # odszukanie nut\n",
    "        dict_keys_time[time] = ','.join(notes.astype(str))\n",
    "        \n",
    "    return dict_keys_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MIDIDataset:\n",
    "    def __init__(self, converter, seq_len=50, song_batch_size=12, nn_batch_size=96):\n",
    "        self.converter = converter\n",
    "        self.seq_len = seq_len\n",
    "        self.song_batch_size = song_batch_size\n",
    "        self.nn_batch_size = nn_batch_size\n",
    "        self._pos = 0\n",
    "        self._song_batch = None\n",
    "        \n",
    "    def get_batch(self):\n",
    "        if self._song_batch is None:\n",
    "            songs_train, songs_target = self.converter.get_batch(self.song_batch_size, self.seq_len)\n",
    "            shuffle_order = np.random.permutation(np.arange(songs_target.shape[0]))\n",
    "            songs_train = songs_train[shuffle_order, :]\n",
    "            songs_target = songs_target[shuffle_order]\n",
    "            self._song_batch = (songs_train, songs_target)\n",
    "            self._pos = 0\n",
    "            \n",
    "        songs_train, songs_target = self._song_batch\n",
    "        end_pos = min(self._pos + self.nn_batch_size, songs_target.shape[0])\n",
    "        train_batch, target_batch = songs_train[self._pos:end_pos, :], songs_target[self._pos:end_pos]\n",
    "        if end_pos == songs_target.shape[0]:\n",
    "            self._song_batch = None\n",
    "        self._pos = end_pos\n",
    "        return train_batch, target_batch\n",
    "    \n",
    "    def generate_sample(self, sample_len, temperature, start_note=None):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "class MIDIConverter:\n",
    "    def __init__(self, directory: str, frac: float=0.1) -> None:\n",
    "        assert 0 < frac <= 1\n",
    "        self._paths = self._locate_midi_files(directory, frac)\n",
    "        self._time_dicts, self._notes_mapping, self._notes_frequency = self._convert_to_time_dicts()\n",
    "        self._inverse_notes_mapping = {v: k for k, v in self._notes_mapping.items()}\n",
    "        \n",
    "    def get_batch(self, batch_size, seq_len):\n",
    "        idx = np.random.choice(len(self._paths), size=batch_size)\n",
    "        batch_train, batch_target = [], []\n",
    "        for i in idx:\n",
    "            time_dict = self._time_dicts[i]\n",
    "            train_vals, target_vals = self._time_dict_to_seq(time_dict, seq_len)\n",
    "            batch_train.append(train_vals)\n",
    "            batch_target.append(target_vals)\n",
    "        return np.vstack(batch_train), np.hstack(batch_target)\n",
    "        \n",
    "    def _time_dict_to_seq(self, time_dict, seq_len) -> np.ndarray:\n",
    "        times = list(time_dict.keys())\n",
    "        start_time, end_time = np.min(times), np.max(times)\n",
    "        n_samples = end_time - start_time\n",
    "        initial_values = [0]*(seq_len-1) + [time_dict[start_time]]\n",
    "        train_values = np.zeros(shape=(n_samples+1, seq_len))\n",
    "        target_values = np.zeros(shape=(n_samples+1))\n",
    "        train_values_per_step = deque(initial_values)\n",
    "        for i in range(n_samples):\n",
    "            train_values[i, :] = list(train_values_per_step)\n",
    "            current_target = time_dict.get(start_time + i, 0)\n",
    "            target_values[i] = current_target\n",
    "            train_values_per_step.popleft()\n",
    "            train_values_per_step.append(current_target)\n",
    "        train_values[n_samples, :] = list(train_values_per_step)\n",
    "        return train_values, target_values\n",
    "        \n",
    "    def _convert_to_time_dicts(self) -> Tuple[Dict[int, Dict[int, str]], Dict[str, int]]:\n",
    "        unique_notes = list()\n",
    "        time_dicts = {}\n",
    "        for i, path in tqdm(enumerate(self._paths), total=len(self._paths)):\n",
    "            pianoroll = midi_path_to_pianoroll(path)\n",
    "            time_dict = pianoroll_to_time_dict(pianoroll)\n",
    "            time_dicts[i] = time_dict\n",
    "            unique_notes += list(time_dict.values())\n",
    "\n",
    "        notes_freq = Counter(unique_notes)\n",
    "        unique_notes = set(unique_notes)\n",
    "        # Replace strings with oridinal encoding\n",
    "        notes_mapping = {note:(i+1) for i, note in enumerate(unique_notes)}\n",
    "        for i, time_dict in time_dicts.items():\n",
    "            for time, notes in time_dict.items():\n",
    "                time_dict[time] = notes_mapping[notes]\n",
    "        return time_dicts, notes_mapping, notes_freq\n",
    "            \n",
    "    def _locate_midi_files(self, base_dir: str, frac: float) -> List[str]:\n",
    "        midi_files = []\n",
    "        for root, dirs, files in os.walk(base_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.midi'):\n",
    "                    midi_files.append(os.path.join(root, file))\n",
    "        return np.random.choice(midi_files, size=int(frac*len(midi_files)), replace=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0423804059f74420a76059804fce4ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=118), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = MIDIConverter('/home/bartek/Datasets/maestro/', frac=0.1)\n",
    "dat = MIDIDataset(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
